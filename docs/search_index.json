[["index.html", "The Researcher’s Guide to Population Genomic Preface About the Author Motivation Acknowledgements Citing this book Disclaimer", " The Researcher’s Guide to Population Genomic Keaka Farleigh 2025-08-11 Preface About the Author Hi! I am an bioinformatician, data scientist, and NSF postdoctoral research fellow in biology Ph.D. candidate, bioinformatician, data scientist, working in the Schield lab at the University of Virginia. I use bioinformatic techniques to understand how genomic and phenotypic variation is influenced by environmental factors. I am particularly interested in local adaptation, hybridization, and the development of new bioinformatic tools for biological applications. Feel free to visit my website or CV if you’d like to check out my past work. Motivation This book was created to help anyone interested in analyzing population genomic data. I have spent many hours in my Ph.D. and post-doc trying to develop and understand scripts to analyze data. I hope that this book makes someone’s life easier and provides a clear and concise explanation of commonly used population genomic analyses. It is a work in progress, so feel free to reach out with any comments, suggestions, or if you would like to contribute a chapter. Acknowledgements I thank my wife, advisors, and collaborators for their support and help with many of the topics discussed in this book. I also acknowledge and thank the National Science Foundation Graduate Research Fellowship Program (NSF GRF Award #2037786), NSF Postdoctoral Research Fellowship in Biology (NSF DBI-2409958) and Miami University for funding my dissertation, post-doctoral fellowship, and for providing the opportunity to develop this book. I also thank the University of Virginia for hosting me during my post-doc. Citing this book If you use any of the materials in this book please use the citation below and remember to cite any packages or data sets as well. Citation Farleigh, K. (2025). The researcher’s guide to population genomics. Disclaimer Please note that this book uses Google Analytics to track the number of website views and traffic data. This data is used for the sole purpose of showing grant agencies (e.g., the National Science Foundation) that the chapters presented here are, in fact, being viewed. "],["double-digest-restriction-site-associated-dna-sequencing-ddradseq.html", "1 Double digest restriction site associated DNA sequencing (ddRADseq) 1.1 Purpose 1.2 Overview 1.3 References", " 1 Double digest restriction site associated DNA sequencing (ddRADseq) Written by Keaka Farleigh on March 3rd, 2024 Last updated by Keaka Farleigh on August 1st, 2025 Reviewed by: Joe Zianno 1.1 Purpose This chapter provides a basic overview of ddRADseq accompanied by visualizations to help researchers understand what is happening at each step in the workflow. If you would like detailed information on the technique, please see Peterson et al. (2012; the original paper). 1.2 Overview ddRADseq is part of the restriction site-associated DNA sequencing family, which are reduced representation sequencing approaches that allow us to generate genome-wide sequence data at a lower cost than whole genome sequencing (Andrews et al., 2016). Generally, these techniques involve DNA digestion with restriction enzyme(s) and attaching adapters to assign DNA fragments to individuals; please see Andrews et al. (2016) for more details. ddRADseq allows us to identify single nucleotide polymorphisms (SNPs) that enable researchers to explore various questions in evolutionary biology, medicine, and many more subjects. ddRADseq involves four or five major steps (depending on if you have extracted DNA or not). First, we extract DNA from tissues, then we digest the DNA with two restriction enzymes (hence the name), followed by adapter ligation, before size-selection, before polymerase chain reaction (PCR), quality-checks, and sequencing (Figure 1). We will walk through each step below. Figure 1. A simplified double digest restriction site-associated DNA sequencing (ddRADseq) workflow. Black rectangles represent DNA, and colored rectangles represent individual-specific or pool-specific adapters. Please note that this figure is a simplified version of ddRADseq and that the process is more complex and involves more than just adding individual-specific and pool-specific adapters (see Liu et al., 2017 for details). 1.2.1 Extraction Extraction is technically not a step in ddRADseq, but we include it because you need DNA input for ddRADseq. Generally, this process involves getting a tissue/blood/scat sample and isolating the DNA. Commonly used methods include phenol-chloroform, Qiagen kits, or a paramagnetic bead protocol. 1.2.2 Digestion Digestion sets ddRADseq apart from other RAD protocols. We use two digestion enzymes to digest the DNA into fragments (Figure 2), while some protocols use one (original RAD) or three (3RAD). Your choice of digestion enzymes is a crucial part of your study. We often choose enzymes that other studies have used with sister species or that we have previously used within the species. Doing so allows us to compare our data with other data generated using the same restriction enzymes. It will be difficult (if not impossible) to compare data if we use different restriction enzymes because we will end up with different random genome fragments. Figure 2. A visualization of digestion in ddRADseq: DNA is digested into fragments using two restriction enzymes. 1.2.3 Ligation Ligation is where you start to feel the heat and have to trust things that you don’t see. You will attach individual adapters to each sample before putting the samples in each pool together in the same tube (Figure 3)! Be sure to use unique adapter tags for individuals in the same pool so that you can tell them apart after sequencing. Note that the number of individuals in each pool is dependent on how many individual adapters you have. Figure 3 A visualization of ligation in ddRADseq: Individual adapters (colored rectangles) are attached to each sample before individuals in the same pool are put in the same tube. 1.2.4 Size-selection Size selection can feel like a black box because studies use different thresholds without explaining why. Size selection filters your DNA fragments in each pool to only contain fragments within a size range (Figure 4). There are a few ways to perform size selection. The old-school method is to physically cut the fragments out of a gel; the other is to use a machine such as a Pippin Prep or Blue Pippin from Sage Science to do it for you. The Pippin methods work by opening and closing a gate. The gate stays open for DNA fragments within your range but is closed when the fragments are outside your range. Choosing the size range can be challenging, and we would like to select a range that contains DNA fragments. One way to help you choose a size range is to run a gel or BioAnalyze the ligated samples to identify the size distribution of your fragments. Another way is to use size ranges from past studies. Figure 4 A visualization of size selection in ddRADseq: we filter out small and large fragments to leave medium fragments. 1.2.5 Polymerase chain reaction (PCR) PCR in ddRADseq is employed to amplify the DNA sequences in our pools. We also add a pool-specific adapter in PCR, allowing us to identify which pool each set of sequences belongs to (Figure 5). The sequencing center generally performs demultiplexing by pool, but this is only possible by adding these pool-specific adapters. Note that you can reuse individual adapters if you have multiple pool adapters because you can first separate the reads by the pool before separating them by individual tags. Just make sure not to repeat combinations of individual-specific and pool-specific adapters! Figure 5 A visualization of PCR in ddRADseq: we amplify sequences and attach pool-specific adapters (blue rectangles). 1.2.6 Sequence Now, we quantify the DNA concentration in our samples, quality-check with a BioAnalyzer or TapeStation, and send the pools for sequencing (Figure 6)! Note that each sequencing center will have its own requirements and services, so be sure to check with them before sending the samples. Figure 6 You’ve done it! Final checks before sequencing. 1.2.6.1 Questions or comments? Please reach out if I can help in any way or if you have questions or any comments. 1.3 References Andrews, K. R., Good, J. M., Miller, M. R., Luikart, G., &amp; Hohenlohe, P. A. (2016). Harnessing the power of RADseq for ecological and evolutionary genomics. Nature Reviews Genetics, 17(2), 81-92. Peterson, B. K., Weber, J. N., Kay, E. H., Fisher, H. S., &amp; Hoekstra, H. E. (2012). Double digest RADseq: an inexpensive method for de novo SNP discovery and genotyping in model and non-model species. PloS one, 7(5), e37135. Liu, M. Y., Worden, P., Monahan, L. G., DeMaere, M. Z., Burke, C. M., Djordjevic, S. P., … &amp; Darling, A. E. (2017). Evaluation of ddRADseq for reduced representation metagenome sequencing. PeerJ, 5, e3837. "],["rna-seq-mapping-pipeline.html", "2 RNA-seq mapping pipeline 2.1 Purpose 2.2 Files required for this pipeline 2.3 Programs used in this pipeline 2.4 Notes on this pipeline 2.5 Overview 2.6 Quality control with FastQC and Trimmomatic 2.7 Mapping with STAR 2.8 Variant calling 2.9 References", " 2 RNA-seq mapping pipeline Written by Keaka Farleigh on April 16th, 2024 Last updated by Keaka Farleigh on April 21st, 2025 Reviewed by: Benjamin A. Akande Tested by: Benjamin A. Akande, Keaka Farleigh 2.1 Purpose This tutorial contains code to perform quality control, mapping, and variant calling of demultiplexed bulk RNA-seq samples. The dataset used in this tutorial is unpublished, but I will link to the associated publication once the manuscript is published. I developed this pipeline for paired-end (PE) sequencing data and tested it with single-end (SE) sequencing data. I note how and where to change commands for SE data. 2.2 Files required for this pipeline Demultiplexed RNA-seq data Genomic fasta file 2.3 Programs used in this pipeline bcftools (Danecek et al., 2021) FastQC (Andrews, 2010) Trimommatic (Bolger et al., 2014) STAR (Dobin et al., 2013) Picard GATK (McKenna et al., 2010; but see this website for additional papers published about GATK) 2.4 Notes on this pipeline This pipeline has been tested on Miami University’s RedHawk High Performace Computing cluster (HPC) and should work elsewhere, but I cannot guarantee it. Additionally, I use batch jobs but have written the pipeline to be interactive (using sh scripts), so reserve compute resources when running this pipeline. Please reach out if you have any questions or need help running the pipeline or working in an HPC environment. Finally, I wrote the for loops in this pipeline to work with fastq, fq.gz, or bam files. In essence, the loops use the file extensions or specific character strings to create a list of files to perform some action on. In most loops, the action is to echo a command into a batch or sh file so that we can save time and do not have to do it ourselves. You can easily adjust these loops by changing the sed command; please reach out if you have any questions about modifying the loop. Let’s get started and analyze some data! 2.5 Overview Analyzing bulk RNA-seq data is complex and feels like a marathon because of the many steps (~14 if you do variant calling). We can lessen our burden by using programs such as STAR (Dobin et al., 2013) and GATK (McKenna et al., 2010). Depending on what analysis you want to do, you may only have to do some of the steps (see Figure 1). Figure 1. RNA-seq analysis workflow. Rounded rectangles indicate steps, and boxes containing multiple rectangles represent programs such as STAR (Dobin et al., 2013) or GATK (McKenna et al., 2010). Pink rectangles indicate analyses that can be performed at specific steps in the workflow. 2.6 Quality control with FastQC and Trimmomatic As with any sequencing data, the first thing that we do is quality control. We employ a two-step quality control procedure. First, we use fastqc to analyze reads and manually inspect the output. Second, we use trimmomatic to perform any necessary read clipping, read filtering, or adapter removal. Assuming that you are in your working directory with all of the demultiplexed files. # Load fastqc if you need module load fastqc # Run fastqc on everything in the directory fastqc * After fastqc runs, we have an HTML report for each file in our directory. Open a file and inspect the output by double-clicking it (this may require you to transfer it to your desktop). A lot is here, but we are most concerned with the per-base sequence quality graph. This graph shows us the quality scores across our reads. We expect that most of the quality scores to fall in the green area (above 28), but sometimes we see bases at the beginning or end of the read fall into the yellow (20-28) or red areas (0-20). I recommend trimming the reads to exclude anything outside of the green areas (below 28), but the decision is yours regarding your data. But how do I trim the reads or remove reads of low-quality? We use trimmomatic to clip reads, remove low-quality reads, and exclude any adapters that are still present. I include example code below for an individual file and code to loop commands into a shell script so that you can be more efficient. The loop first lists any file with _1.fq.gz using ls, which we assume are all your forward reads. We then isolate the sample names to act as a prefix by removing the _1.fq.gz with sed. We then use this as a prefix to name our input and output files rather than doing it by hand. You must remove any _2 code to make this loop work for SE data. ### Commands to trim a single individual/file # Trimmomatic requires java module load java # Make a trimmed directory to keep our working directory clean mkdir ./trimmed # Run the trimmomatic command java -jar /home/farleik/Software/trimmomatic-0.39.jar PE Sample1_1.fq.gz\\ Sample1_2.fq.gz\\ ./trimmed/Sample1_1_paired.fq.gz\\ ./trimmed/Sample1_1_unpaired.fq.gz\\ ./trimmed/Sample1_2_paired.fq.gz ./trimmed/Sample1_2_unpaired.fq.gz ILLUMINACLIP:TruSeq3-PE.fa:2:30:10:2:keepBothReads LEADING:3 TRAILING:3 MINLEN:36 ### Loop it into a shell file for efficiency # This command will create a file named trimmomatic.sh in your working directory touch trimmomatic.sh # Trimmomatic requires java so we add the command to load java to the trimmomatic.sh file echo module load java &gt;&gt; trimmomatic.sh # We don’t want to clutter up our working directory so we need to create a directory that holds all of the output echo mkdir ./trimmed &gt;&gt; trimmomatic.sh # The for loop to generate the trimmomatic command for each individual/file for i in `ls -1 *_1.fq.gz | sed &#39;s/\\_1.fq.gz//&#39;`; do echo java -jar /home/farleik/Software/trimmomatic-0.39.jar PE ${i}_1.fq.gz\\ ${i}_2.fq.gz\\ ./trimmed/${i}_1_paired.fq.gz\\ ./trimmed/${i}_1_unpaired.fq.gz\\ ./trimmed/${i}_2_paired.fq.gz ./trimmed/${i}_2_unpaired.fq.gz ILLUMINACLIP:TruSeq3-PE.fa:2:30:10:2:keepBothReads LEADING:3 TRAILING:3 MINLEN:36 &gt;&gt; trimmomatic.sh; done # Trim everything sh trimmomatic.sh Now that everything is trimmed we can move onto mapping with STAR! 2.7 Mapping with STAR Mapping the RNA-seq data with STAR is relatively straightforward (it only requires a couple of commands). Still, I encourage you to familiarize yourself with the program by checking out the manual. The options I use here are specific for my project. While they may also be relevant to your project, it is best to understand your chosen settings. First, we will use our genomic fasta to generate some files used by STAR during the mapping process. We then use a for loop similar to the trimmomatic step where we first isolate a prefix and then use that prefix to name input and output files to map and generate read counts for each individual. Note. GFF read is a part of cufflinks. Please see their GitHub for relevant citations. # Load STAR module load star-2.7.5a # Converting GFF to GTF ./gffread Phry_platy.gff -T -o Phry_platy_ann.gtf # Use the genome fasta to generate files used by STAR during mapping STAR --runMode genomeGenerate --genomeDir ./indexes/ --genomeFastaFiles ../../redo_annotation/final_assembly.fa --sjdbGTFfile ./Phry_platy_ann.GTF # This command will create a file named RNAseq_mapping.sh in your working directory touch RNAseq_mapping.sh # Add modulel load command into the shell script echo module load star-2.7.5a &gt;&gt; RNAseq_mapping.sh # This command is for PE data for i in `ls -1 *_1_paired.fq.gz | sed &#39;s/\\_1_paired.fq.gz//’ `; do echo STAR --runThreadN 2 --genomeDir /shared/jezkovt_farleik_shared/Platy_RNAseq/Genome/ --readFilesIn ${i}_1_paired.fq.gz ${i}_2_paired.fq.gz --readFilesCommand zcat --limitOutSJcollapsed 2000000 --outFileNamePrefix ./${i} --outSAMtype BAM SortedByCoordinate --limitBAMsortRAM 29000000000 --quantMode GeneCounts &gt;&gt; RNAseq_mapping.sh; done sh RNAseq_mapping.sh # This command is for SE data, notice that there is only one input file (--readFilesIn) for i in `ls -1 *.fastq | sed &#39;s/\\.fastq//’ `; do echo STAR --runThreadN 2 --genomeDir /shared/jezkovt_farleik_shared/Platy_RNAseq/Anolis_Data/Genome/ --readFilesIn ${i}.fastq --readFilesCommand zcat --limitOutSJcollapsed 2000000 --outFileNamePrefix ./${i} --outSAMtype BAM SortedByCoordinate --limitBAMsortRAM 29000000000 --quantMode GeneCounts &gt;&gt; Anolis_IndMapping.sh; done Mapping is done, well, maybe. We can use the files output here to perform differential gene expression analysis or weighted gene co-expression analysis analyses. However, if we want to call variants in our RNA-seq data, I recommend following GATK’s best practices and using STAR’s two-pass method. This method increases sensitivity to novel splice junctions, and in the absence of annotations, it is strongly recommended. We will use the splice junction files generated by STAR (SJ.out.tab files) to re-generate our genome indices. First, we filter out potential false positive splice junctions with few reads, non-canonical junctions, and annotated junctions (see this discussion). # I like to move all of the files to a new directory to be safe mkdir SJ_files mv *SJ.out.tab ./SJ_files cd ./SJ_files # Filter our splice junction files cat *.tab | awk &#39;($5 &gt; 0 &amp;&amp; $7 &gt; 2 &amp;&amp; $6==0)&#39; | cut -f1-6 | sort | uniq &gt; SJ_out_filtered.tab cd ../ Now we can re-generate our genome indices. We will use this for all files in the second pass. # Generate genome indices again mkdir GenomeSecondPass STAR --genomeDir ./GenomeSecondPass/ --runMode genomeGenerate --genomeFastaFiles /shared/jezkovt_farleik_shared/Platy_RNAseq/Genome/GCA_020142125.1_MUOH_PhPlat_1.1_genomic.fna --sjdbGTFfile /shared/jezkovt_farleik_shared/Platy_RNAseq/Genome/*.gtf --sjdbFileChrStartEnd ./SJ_files/SJ_out_filtered.tab --runThreadN 2 Finally, we will perform the second pass mapping. The loops use the same format as above. # Make a new directory to be tidy and not modify any of the files generated during step 1 mkdir SecondPass cp *.fq.gz ./SecondPass cd ./SecondPass # Create the new file for 2nd pass mapping touch Platy_IndMapping_2ndPass.sh # Echo the STAR module into it echo module load star-2.7.5a &gt;&gt; Platy_IndMapping_2ndPass.sh # Map the reads again to the second pass genome file, this is for PE data for i in `ls -1 *_1_paired.fq.gz | sed &#39;s/\\_1_paired.fq.gz//&#39; `; do echo STAR --runThreadN 2 --genomeDir /shared/jezkovt_farleik_shared/Platy_RNAseq/SecondPass/GenomeSecondPass/ --readFilesIn ${i}_1_paired.fq.gz ${i}_2_paired.fq.gz --readFilesCommand zcat --limitOutSJcollapsed 2000000 --outFileNamePrefix ./${i}_2ndPass --outSAMtype BAM SortedByCoordinate --limitBAMsortRAM 29000000000 --quantMode GeneCounts &gt;&gt; Platy_IndMapping_2ndPass.sh; done # Command for SE data for i in `ls -1 *.fastq.gz | sed &#39;s/\\.fastq.gz//&#39; `; do echo STAR --runThreadN 2 --genomeDir /shared/jezkovt_farleik_shared/Platy_RNAseq/SecondPass/GenomeSecondPass/ --readFilesIn ${i}.fastq.gz --readFilesCommand zcat --limitOutSJcollapsed 2000000 --outFileNamePrefix ./${i}_2ndPass --outSAMtype BAM SortedByCoordinate --limitBAMsortRAM 29000000000 --quantMode GeneCounts &gt;&gt; Anolis_IndMapping_2ndPass.sh; done 2nd pass mapping is done! We can move onto variant calling with the GATK suite of tools. You can also stop here for differential gene expression, alternative splicing, and more analyses (see Figure 1). 2.8 Variant calling Variant calling is more complicated and time-consuming than mapping, but it is doable, especially since we will use loops to do most of our work! Note. We do not have any differences in commands for PE and SE data from here forward. 2.8.1 Variant calling steps Add read group tags Duplicate identification Dictionary creation Cigar splitting Preliminary site calling Base recalibration Sample naming Variant filtering 1. We use picard to add read group tags to our reads. We add read groups tags because many programs assume that there is the presence of read group tags and will not work without them. Note, that picard will need to be downloaded from the hyperlink listed above. See the AddorReplaceReadGroups page for more details. Note that the loop has the same logic as the ones before, but we replace _2ndPassAligned.sortedByCoord.out.bam instead of _1.fq.gz. # Make the directory mkdir Add_Group # Create the script to hold the commands for each file touch Platy_AddGroups.sh # We need a specific version of java echo module load java-20 &gt; Platy_AddGroups.sh for i in `ls -1 *2ndPassAligned.*.bam | sed &#39;s/\\_2ndPassAligned.sortedByCoord.out.bam//&#39; `; do echo java -jar /home/farleik/Software/picard.jar AddOrReplaceReadGroups I= ${i}_2ndPassAligned.sortedByCoord.out.bam O= ./Add_Group/${i}_Add_Group.bam RGID=4 RGLB=lib1 RGPL=illumina RGPU=identifier RGSM=sample_name &gt;&gt; Platy_AddGroups.sh; done # Run it sh Platy_AddGroups.sh cd ./Add_Group 2. We identify any duplicate reads with picard. Picard identifies duplicates by comparing the 5 prime positions of reads and then differentiates the primary read from the duplicates using the sums of the base-quality scores. Duplicate reads are labelled in the output file. See the MarkDuplicates page for more details. # Make a directory to hold the duplicated marked reads mkdir Dup_filtered # Echo the java module we need into a new shell script echo module load java-20 &gt; Platy_DupFiltering.sh # Create a for loop to do the work for us for i in `ls -1 *Add_Group*.bam | sed &#39;s/\\_Add_Group.bam//&#39; `; do echo java -jar /home/farleik/Software/picard.jar MarkDuplicates I= ${i}_Add_Group.bam O= ./Dup_filtered/${i}_DupFiltered.bam CREATE_INDEX=true VALIDATION_STRINGENCY=SILENT M= ./Dup_filtered/${i}_DupFiltered_stats.txt &gt;&gt; Platy_DupFiltering.sh; done # Run the sh script sh Platy_DupFiltering.sh # Change into the directory after you are done cd ./Dup_filtered 3. Create a dictionary from our reference sequence with picard. Many tools require this step’s dictionary, which will be used in subsequent steps. See the CreateSequenceDictionary page for more details. java -jar /home/farleik/Software/picard.jar CreateSequenceDictionary -R /shared/jezkovt_farleik_shared/Platy_RNAseq/Genome/GCA_020142125.1_MUOH_PhPlat_1.1_genomic.fna -O Platy_ref.dict 4. Now we will use GATK to split Cigar N reads and reassign quality scores. This tool will split reads that contain Ns in their cigar string. For RNAseq data, these are reads that span splice events. See the SplitNCigarReads page for details. # Make directory to hold output mkdir Cigar_Split # Echo module into new shell script echo module load gatk-4.1.2.0 &gt; Platy_CigarSplit.sh # Create a for loop for i in `ls -1 *Dup*.bam | sed &#39;s/\\_DupFiltered.bam//&#39; `; do echo gatk SplitNCigarReads -R Platy.fna -I ${i}_DupFiltered.bam -O ./Cigar_Split/${i}_CigarSplit.bam &gt;&gt; Platy_CigarSplit.sh; done # Run it sh Platy_CigarSplit.sh # Change into Cigar split directory cd ./Cigar_Split 5. Preliminary variant calling. We perform this step because our organism is a non-model organism, and we do not know which sites are polymorphic. We need this “known sites” file to perform base recalibration. You can skip this step if you already have a “known sites” file. The known sites file is essential because the base recalibration algorithm treats mismatches as errors, but we expect variation at polymorphic sites, so we use this file to skip over these sites. You need a “known sites” file regardless of whether you are working with a model or non-model organism, so you should run preliminary variant calling if you do not have a “known sites” file. # Create directory mkdir ./Prelim_VCFcalling # Echo command into shell script echo module load gatk-4.1.2.0 &gt; Platy_PrelimVCFcalling.sh # Create a loop to perform prelim variant calling for i in `ls -1 *Cigar*.bam | sed &#39;s/\\_CigarSplit.bam//&#39; `; do echo gatk HaplotypeCaller -R ../Platy.fna -I ${i}_CigarSplit.bam -O ./Prelim_VCFcalling/${i}_Prelim.g.vcf.g.gz -ERC GVCF &gt;&gt; Platy_PrelimVCFcalling.sh; done # Run it sh Platy_PrelimVCFcalling.sh # Change into the directory cd ./Prelim_VCFcalling # Combine the gVCFs into one multisample vcf ls *vcf.g.gz &gt; vcfs.list gatk CombineGVCFs -R /shared/jezkovt_farleik_shared/Platy_RNAseq/SecondPass/Add_Group/Dup_filtered/Platy.fna --variant vcfs.list -O Prelim_Platy.g.vcf.gz 6. Base recalibration. Base recalibration is a critical step. Base quality scores represent per-base estimates of error calculated during sequencing. These scores can be subject to different types of errors that influence base quality score estimates. The recalibration tool employs a machine learning model to adjust scores to make the overall base quality scores more accurate. See this article for details. # Recalibrate base quality scores echo module load gatk-4.1.2.0 &gt; Platy_BQSR.sh # For loop for multiple files for i in `ls -1 *Cigar*.bam | sed &#39;s/\\_CigarSplit.bam//&#39; `; do echo gatk BaseRecalibrator -R ../Platy.fna -I ${i}_CigarSplit.bam -–known-sites ./Prelim_VCFcalling/Prelim_Platy.g.vcf.gz -O ${i}_recalibration.table &gt;&gt; Platy_BQSR.sh; done sh Platy_BQSR.sh # Apply the recalibration mkdir ./Recalibrated_Files echo module load gatk-4.1.2.0 &gt; Platy_ABQSR.sh # Loop for multiple files for i in `ls -1 *Cigar*.bam | sed &#39;s/\\_CigarSplit.bam//&#39; `; do echo gatk ApplyBQSR -R ../Platy.fna -I ${i}_CigarSplit.bam –bqsr-recal-file ./${i}_recalibration.table -O ./Recalibrated_Files/${i}_Recalibrated.bam &gt;&gt; Platy_ABQSR.sh; done sh Platy_ABQSR.sh 7. Sample Naming. You would think that the VCFs have sample names associated with them because we have named the files, right? Unfortunately, not. We need to add sample names to each VCF so that GATK knows which genetic data is associated with each individual when we combine the VCF files for variant calling. # Echo java into new shell script echo module load java-20 &gt; Platy_SampleNaming.sh # Create a for loop for i in `ls *.g.vcf.g.gz | sed &#39;s/\\.g.vcf.g.gz//&#39;`; do echo java -jar /home/farleik/Software/picard.jar RenameSampleInVcf --INPUT ${i}.g.vcf.g.gz --OUTPUT ${i}_sampnamed.g.vcf.g.gz --NEW_SAMPLE_NAME ${i} &gt;&gt; Platy_SampleNaming.sh; done # Run it sh Platy_SampleNaming.sh # Create indexes for each sample named file for file in *_sampnamed.g.vcf.g.gz; do gatk IndexFeatureFile -F ${file} -O ${file}.tbi; done 8. Variant filtering. Finally, we get to the fun part: variant filtering. All of the data pre-processing steps have led to this, and we only have a couple more to go before we have a file ready for analysis. See this article for a detailed discussion of how we call SNPs with GATK. I prefer to use bcftools to perform variant filtering but you can also use GATK. ### Combine into a multi-sample vcf # List all files to combine ls *_sampnamed.g.vcf.g.gz &gt; vcfs.list gatk CombineGVCFs -R /shared/jezkovt_farleik_shared/Platy_RNAseq/Platy_Data/SecondPass/Add_Group/Dup_filtered/Platy.fna --variant vcfs.list -O Platy_wsamps.g.vcf.gz ### Genotype vcfs gatk GenotypeGVCFs -R ../Platy.fna -V Platy_wsamps.g.vcf.gz -O Platy_gtyped_wsamps.vcf.gz mkdir ./VCFs_wnames mv *_sampnamed.g.vcf.* /VCFs_wnames/ cd VCFs_wnames/ # Make a directory for variant filtering mkdir ./VariantFiltering module load bcftools-1.15 # Select the biallelic SNPs only bcftools view -m2 -M2 -v snps Platy_gtyped_wsamps.vcf.gz -o ./VariantFiltering/Platy_SNPs_only.vcf.gz # Apply the hard filters from gatk bcftools view -e &#39;QD &lt; 2.0 &amp; QUAL &lt; 30 &amp; FS &gt; 60 &amp; SOR &lt; 3 &amp; MQ &lt; 40 &amp; MQRankSum &lt; -12.5 &amp; ReadPosRankSum &lt; -8 &#39; Platy_SNPs_only.vcf.gz -o Platy_SNPs_only_filtered.vcf.gz # Additional filtering on depth, missing data, and minor allele frequency with vcftools vcftools --gzvcf Platy_SNPs_only_filtered.vcf.gz --minDP 5 --max-missing 0.75 --maf 0.05 Congrats, you did it! You can use this vcf for various analyses, including variant effect prediction and sliding window statistics (see Figure 1). Please reach out if you find any bugs in the code or have any questions or suggestions on chapter or book additions. 2.9 References Andrews, S. (2010). FastQC: A Quality Control Tool for High Throughput Sequence Data [Online]. Available online at: http://www.bioinformatics.babraham.ac.uk/projects/fastqc/ Bolger, A. M., Lohse, M., &amp; Usadel, B. (2014). Trimmomatic: a flexible trimmer for Illumina sequence data. Bioinformatics, 30(15), 2114-2120. Danecek P, Bonfield JK, et al. Twelve years of SAMtools and BCFtools. Gigascience (2021) 10(2):giab008 Dobin, A., Davis, C. A., Schlesinger, F., Drenkow, J., Zaleski, C., Jha, S., … &amp; Gingeras, T. R. (2013). STAR: ultrafast universal RNA-seq aligner. Bioinformatics, 29(1), 15-21. McKenna A, Hanna M, Banks E, Sivachenko A, Cibulskis K, Kernytsky A, Garimella K, Altshuler D, Gabriel S, Daly M, DePristo MA. (2010). The Genome Analysis Toolkit: a MapReduce framework for analyzing next-generation DNA sequencing data. Genome Res, 20:1297-303. DOI: 10.1101/gr.107524.110. "],["introduction-to-r.html", "3 Introduction to R 3.1 Purpose 3.2 Files required for this pipeline 3.3 Programs used in this pipeline 3.4 Notes on this tutorial 3.5 Overview 3.6 What is R? 3.7 What is Rstudio? 3.8 Advantages and disadvantges of R 3.9 References", " 3 Introduction to R Written by Keaka Farleigh on August 5th, 2025: edited from materials originally developed by Dr. Tereza Jezkova and Alfredo Ascanio Last updated by Keaka Farleigh on August 8th, 2025 3.1 Purpose This tutorial will provide an introduction to the programming language R. I will assume that you have no experience with R and that you are at the beginning of your journey towards becoming an R expert. 3.2 Files required for this pipeline None! 3.3 Programs used in this pipeline R (R core team, 2025) Rstudio 3.4 Notes on this tutorial The materials in this tutorial were originally developed as part of the Lambda bioinformatic workshop at Miami University by Dr. Tereza Jezkova and Alfredo Ascanio. These materials are now presented at Lambda and the Foundations in R workshop at the University of Virginia. 3.5 Overview We will cover the basics of R and Rstudio before getting some hands-on experience! 3.6 What is R? R is a free and open-source (public) programming language that was developed in the 1990’s and published in 2000. It is based on the programming language S, which was developed by Bell labs in the 1970’s. I list some basic facts about R below: R can be run on any platform (e.g., Linux, Mac, Windows). R has public repositories that hold software that can be downloaded to perform a variety of analyses. These repositories are maintained by volunteers and require software to be documented and adhere to specific rules to be held in the repository. The official repository is the CRAN. The other major repository is BioConductor. R software can also be hosted and downloaded from other repositories such as GitHub, but these repositories do not require specific documentation or require developers to adhere to specific practices (some still do). There is (and has been) a large community of R users, so there are a lot of helpful resources online if you have questions or run into any errors. 3.7 What is Rstudio? Rstudio is an integrated development environment (IDE) for R. Rstudio is also free and open-source. So what is an IDE? An IDE is a software application that allows you to run analyses, develop software, debug code, and more all in a single interface. Rstudio is often used to run R, because, well, R was developed in the 1990’s and it looks like it. Rstudio allows us to have a more streamlined interface, see everything that we are working on, actively edit our code, and visualize plots as we go. It is more difficult to do these things in base R. 3.8 Advantages and disadvantges of R 3.9 References "],["data-wrangling-with-r.html", "4 Data wrangling with R 4.1 Purpose 4.2 Files required for this pipeline 4.3 Programs used in this pipeline 4.4 Notes on this tutorial 4.5 Overview 4.6 What is data wrangling? 4.7 References", " 4 Data wrangling with R Written by Keaka Farleigh on August 5th, 2025 Last updated by Keaka Farleigh on August 11th, 2025 4.1 Purpose This tutorial will give you an introduction to data wrangling. You will learn what it is and get some hands-on experience wrangling some data. I will assume that you have minimal experience with R and are a beginner user. Please read through the Introduction to R chapter or send me an email if you do not feel comfortable with this tutorial. 4.2 Files required for this pipeline None! 4.3 Programs used in this pipeline R (R core team, 2025) Rstudio 4.4 Notes on this tutorial The materials in this tutorial were originally developed as part of the Lambda bioinformatic workshop at Miami University by Dr. Tereza Jezkova and Alfredo Ascanio. These materials are now presented at Lambda and the Foundations in R workshop at the University of Virginia. 4.5 Overview We will cover the basics of data wrangling before working through some examples! 4.6 What is data wrangling? Have you ever opened a file from a collaborator or output from a program but it was not formatted as you expected? What did you do? Did you leave it as is, or did you modify the file so that it conformed to your expectations? If you modified the file, then you have already wrangled some data. Data wrangling is a fancy way of saying data processing, transformation, and cleaning to prepare it for analysis, presentation, or visualization. For many researchers and data scientists, data wrangling consumes a lot of time and is therefore a foundation tool for modern data analysis. 4.7 References "],["data-analysis-and-visualization-in-r.html", "5 Data analysis and visualization in R 5.1 Purpose 5.2 Files required for this pipeline 5.3 Programs used in this pipeline 5.4 Notes on this tutorial 5.5 Overview 5.6 References", " 5 Data analysis and visualization in R Written by Keaka Farleigh on August 5th, 2025 Last updated by Keaka Farleigh on August 5th, 2025 5.1 Purpose 5.2 Files required for this pipeline 5.3 Programs used in this pipeline R (R core team, 2025) Rstudio 5.4 Notes on this tutorial 5.5 Overview 5.6 References "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
